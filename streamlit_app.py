import streamlit as st
import spacy
import subprocess
import importlib

# spaCyãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå¿…è¦ãªã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
@st.cache_resource
def load_spacy_model():
    try:
        return spacy.load("en_core_web_sm")
    except OSError:
        st.warning("spaCy ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã§ã™...")
        subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
        importlib.invalidate_caches()
        return spacy.load("en_core_web_sm")

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
nlp = load_spacy_model()

# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
sample_text = """John reads books.
Mary likes coffee.
The cat chased the mouse in the garden.
Students are studying mathematics.
She gave him a present."""

# UI - ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("ğŸ” è‹±æ–‡SVOæŠ½å‡ºã‚¢ãƒ—ãƒª")
st.markdown("è‹±æ–‡ã‹ã‚‰**ä¸»èª(Subject)ãƒ»å‹•è©(Verb)ãƒ»ç›®çš„èª(Object)**ã‚’è‡ªå‹•æŠ½å‡ºã—ã¾ã™")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("ğŸ“– ä½¿ã„æ–¹")
    st.markdown("""
    1. è‹±æ–‡ã‚’å…¥åŠ›æ¬„ã«å…¥åŠ›  
    2. ã€ŒSVOã‚’æŠ½å‡ºã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯  
    3. æŠ½å‡ºã•ã‚ŒãŸSVOæ§‹é€ ã‚’ç¢ºèª
    
    ### å¯¾å¿œã™ã‚‹æ–‡ã®ä¾‹ï¼š
    - John reads books.
    - Mary likes coffee.
    - The cat chased the mouse.
    """)
    
    st.header("â„¹ï¸ About")
    st.markdown("""
    ã“ã®ã‚¢ãƒ—ãƒªã¯spaCyã®è‡ªç„¶è¨€èªå‡¦ç†ã‚’ä½¿ç”¨ã—ã¦  
    è‹±æ–‡ã®æ§‹é€ ã‚’è§£æã—ã¦ã„ã¾ã™ã€‚
    
    [GitHub](https://github.com/yourusername/yourrepo)
    """)

# ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ¬„
text = st.text_area(
    "è‹±æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", 
    value=sample_text,
    height=150,
    help="è‹±æ–‡ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€SVOæ§‹é€ ã‚’æŠ½å‡ºã—ã¾ã™"
)

# SVOæŠ½å‡ºå‡¦ç†
def extract_svo(doc):
    """SVOæŠ½å‡ºé–¢æ•°"""
    svos = []
    for sent in doc.sents:
        for token in sent:
            if token.pos_ == "VERB" or token.dep_ == "ROOT":
                subject = None
                verb = token.text
                obj = None
                
                for child in token.children:
                    if "subj" in child.dep_:
                        subject = get_compound_phrase(child)
                    elif child.dep_ == "dobj":
                        obj = get_compound_phrase(child)
                    elif child.dep_ == "pobj" and obj is None:
                        obj = get_compound_phrase(child)
                
                verb_phrase = get_verb_phrase(token)
                if verb_phrase:
                    verb = verb_phrase
                
                if subject and verb and obj:
                    svos.append((subject, verb, obj))
    return svos

def get_compound_phrase(token):
    """è¤‡åˆèªå¥ã‚’å«ã‚€ä¸»èªã‚„ç›®çš„èªã®å®Œå…¨ãªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å–å¾—"""
    phrase_parts = []

    for child in token.children:
        if child.dep_ in ["compound", "amod", "det", "poss"] and child.i < token.i:
            phrase_parts.append(child.text)

    phrase_parts.append(token.text)

    for child in token.children:
        if child.dep_ in ["compound", "prep"] and child.i > token.i:
            phrase_parts.append(child.text)

    return " ".join(phrase_parts)

def get_verb_phrase(token):
    """åŠ©å‹•è©ã‚’å«ã‚€å‹•è©å¥å…¨ä½“ã‚’å–å¾—"""
    verb_parts = []
    for child in token.children:
        if child.dep_ in ["aux", "auxpass"] and child.i < token.i:
            verb_parts.append(child.text)
    verb_parts.append(token.text)
    return " ".join(verb_parts) if len(verb_parts) > 1 else None

# ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§è©³ç´°è¡¨ç¤º
show_debug = st.checkbox("è©³ç´°ãªè§£æçµæœã‚’è¡¨ç¤º", value=False)

# æŠ½å‡ºãƒœã‚¿ãƒ³
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    extract_button = st.button("ğŸ¯ SVOã‚’æŠ½å‡º", type="primary", use_container_width=True)

# ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã¨ãã®å‡¦ç†
if extract_button:
    if text:
        with st.spinner('è§£æä¸­...'):
            doc = nlp(text)

            if show_debug:
                st.subheader("ä¾å­˜æ§‹é€ è§£æçµæœï¼š")
                for sent in doc.sents:
                    with st.expander(f"æ–‡: {sent.text[:50]}..."):
                        debug_data = []
                        for token in sent:
                            debug_data.append({
                                "å˜èª": token.text,
                                "å“è©": token.pos_,
                                "ä¾å­˜é–¢ä¿‚": token.dep_,
                                "è¦ª": token.head.text if token.head != token else "ROOT"
                            })
                        st.table(debug_data)

            svos = extract_svo(doc)

            if svos:
                st.subheader("âœ¨ æŠ½å‡ºã•ã‚ŒãŸSVOæ§‹é€ ")
                for i, (s, v, o) in enumerate(svos, 1):
                    with st.container():
                        st.markdown(f"### æ–‡ {i}")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ä¸»èª (S)", s)
                        with col2:
                            st.metric("å‹•è© (V)", v)
                        with col3:
                            st.metric("ç›®çš„èª (O)", o)
                        st.markdown("---")
                st.success(f"âœ… {len(svos)}å€‹ã®SVOæ§‹é€ ã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼")
            else:
                st.warning("âš ï¸ SVOæ§‹é€ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.info("""
                ğŸ’¡ **ãƒ’ãƒ³ãƒˆ:**  
                - å®Œå…¨ãªSVOæ§‹é€ ã‚’æŒã¤æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„  
                - ä¾‹: "John reads books." "She loves music."
                """)
    else:
        st.error("âŒ è‹±æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center'>
        <small>Made with â¤ï¸ using Streamlit and spaCy</small>
    </div>
    """,
    unsafe_allow_html=True
)
